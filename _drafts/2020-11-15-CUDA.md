---
layout: post
title: CUDA
date:  2020-11-15 00:00:00 -1000
categories:
---

* TOC
{:toc}

# Nsight Compute

* Create Nvidia developers account
* Download Nsight Compute: nsight-compute-linux-2020.2.1.8-29181059.run 
* sudo sh ./nsight-compute-linux-2020.2.1.8-29181059.run 
* result:

	```
tom@zen:~$ ll -d /usr/local/NVIDIA-Nsight-Compute*
lrwxrwxrwx 1 root root   39 Nov 15 21:29 /usr/local/NVIDIA-Nsight-Compute -> /usr/local/NVIDIA-Nsight-Compute-2020.2/
drwxr-xr-x 7 root root 4096 Nov 15 21:29 /usr/local/NVIDIA-Nsight-Compute-2020.2/
	```
* Run: `/usr/local/NVIDIA-Nsight-Compute/nv-nsight-cu`
* Load any cuda application
* enable "Auto Profile"
* Run to kernel -> statistics

**Need to enable permissions to allow GPU counter or run as sudo**.

# Interesting CUDA topics

* [CUDA Dynamic Parallelism](https://developer.nvidia.com/blog/cuda-dynamic-parallelism-api-principles/)

	One CUDA kernel kicks off multiple others.

	Examples are in ./NVIDIA_CUDA-11.1_Samples/0_Simple/cdp*

	Examples are in ./NVIDIA_CUDA-11.1_Samples/6_Advanced/cdp*


* [CUDA Cooperative Groups](https://developer.nvidia.com/blog/cooperative-groups/)

	Abstraction layer to synchronize between different kinds of thread groups.

	Used in many CUDA examples. (grep for "cooperative_groups".)

* [CUDA Asynchronous Command Streams](https://developer.nvidia.com/blog/gpu-pro-tip-cuda-7-streams-simplify-concurrency/)

	Shows how to run multiple CUDA kernels in parallel.

	See also ./NVIDIA_CUDA-11.1_Samples/6_Advanced/concurrentKernels

	See also ./NVIDIA_CUDA-11.1_Samples/6_Advanced/StreamPriorities

* [How to Overlap Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-overlap-data-transfers-cuda-cc/) and
   [How to Optimize Data Transfers in CUDA C/C++](https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/)

	Discusses CUDA streams and asynchronous memory transfers, with a shoutout to HyperQ.

* Parallel Reduction example

	See ./NVIDIA_CUDA-11.1_Samples/6_Advanced/reduction

	Sum all the values of a large array

* [Parallel Prefix Sum (Scan) with CUDA](https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-39-parallel-prefix-sum-scan-cuda)

	Explains about how to build large-scale scan operation on a G80 class GPU.

	Then goes on to describe applications for scan operations: 

	* stream compaction (filtering) of an array

		* Create array with a '1' for all elements that need to be retrained
		* Use scan operation on this array
		* the result is an array with the address/index where the retained elements need to be stored using a scatter operation

	* Summed array tables

		Used to implement variable width box filters on an image, e.g. for depth of field blurring
		effects.

	* Radix Sort

		Primitive element is the 'split' primitive: use the scan operation to count the number
		of elements that have a particular bit set to 0.

	Original paper: [Scans as Primitive Parallel Operations](https://pdfs.semanticscholar.org/3c01/9693f59a32bf2fe5e99e93372c2816705139.pdf) by
	Guy E. Blelloch. Even better by same author: [Prefix Sums and Their Applications](https://www.cs.cmu.edu/~guyb/papers/Ble93.pdf).


	See ./NVIDIA_CUDA-11.1_Samples/6_Advanced/scan : uses shared memory only.

	See ./NVIDIA_CUDA-11.1_Samples/6_Advanced/shfl_scan : uses shfl intrinsic for initial intra-warp scan.

* Scalar Vector Vector multiplication

	See ./NVIDIA_CUDA-11.1_Samples/6_Advanced/scalarProd

	Very basic example of using shared memory to store temporary accumulation variables before
	reduction.
	
* [Faster Parallel Reductions on Kepler ](https://developer.nvidia.com/blog/faster-parallel-reductions-kepler/)

	Talks about the `shfl` intrinics for faster parallel reduction.

	The `shfl` instructions make it possible to avoid shared memory by reading a register from a different thread
	within the same warp.

	See also: [Do the Kepler Shuffle](https://developer.nvidia.com/blog/cuda-pro-tip-kepler-shuffle/) video.

* [Nvidia HPC SDK](https://developer.nvidia.com/hpc-sdk)

	Has automatically parallelizing C++ compiler. Must be installed in addition to CUDA.

	[Accelerating Standard C++ with GPUs Using stdpar](https://developer.nvidia.com/blog/accelerating-standard-c-with-gpus-using-stdpar/)
	shows how to use it.

	Only works for Volta and up.

* [Controlling Data Movement to Boost Performance on the NVIDIA Ampere Architecture](https://developer.nvidia.com/blog/controlling-data-movement-to-boost-performance-on-ampere-architecture/)

	Asynchronous copies straight from L2 into shared memory 

* [Getting Started with CUDA Graphs](https://developer.nvidia.com/blog/cuda-graphs/)

	Create network of CUDA kernels that execute automatically in the right order.

* [CUDA on NVIDIA Ampere GPU Architecture: Taking Your Algorithms to the Next Level of Performance](https://www.nvidia.com/en-us/gtc/session-catalog-details/?search=S21170&ncid=em-sfdc-84120)

	wait/arrive barriers etc.

* [Introducing Low-Level GPU Virtual Memory Management](https://developer.nvidia.com/blog/introducing-low-level-gpu-virtual-memory-management/)

	Resizable memory.

* [CUDA on Turing Opens New GPU Compute Possibilities](https://developer.nvidia.com/blog/cuda-turing-new-gpu-compute-possibilities/)
	
	Interesting talk with example that builds a Trie.

